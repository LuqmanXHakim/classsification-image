# -*- coding: utf-8 -*-
"""Luqman Hakim_Submission_Akhir_Klasifikasi Gambar.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ems26Ox_CKzdvr7hKnX-_NtxA7Shh-q7

# Proyek Klasifikasi Gambar: Flowers Dataset
- **Nama:** Luqman Hakim
- **Email:** luqmanxhakim22042002@gmail.com
- **ID Dicoding:** 2608610

Sumber Dataset: https://www.kaggle.com/datasets/imsparsh/flowers-dataset

## Import Semua Packages/Library yang Digunakan
"""

!pip install tensorflowjs

# Commented out IPython magic to ensure Python compatibility.
# Mengimpor libraries umum yang sering digunakan
import os, shutil
import zipfile
import random
from random import sample
import shutil
from shutil import copyfile
import pathlib
from pathlib import Path
import numpy as np
import pandas as pd
import seaborn as sns
from tqdm.notebook import tqdm as tq

# Mengimpor libraries untuk visualisasi
# %matplotlib inline
import matplotlib.image as mpimg
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.image import imread

# Mengimpor libraries untuk pemrosesan data gambar
import cv2
from PIL import Image
import skimage
from skimage import io
from skimage.transform import resize
from skimage.transform import rotate, AffineTransform, warp
from skimage import img_as_ubyte
from skimage.exposure import adjust_gamma
from skimage.util import random_noise

# Mengimpor libraries untuk pembuatan dan evaluasi model
import keras
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.utils.class_weight import compute_class_weight
import tensorflow as tf
from tensorflow.keras import Model, layers
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img
from tensorflow.keras.optimizers import Adam, RMSprop, SGD
from tensorflow.keras.applications import VGG16
from tensorflow.keras.layers import InputLayer, Conv2D, SeparableConv2D, MaxPooling2D, MaxPool2D, Dense, Flatten, Dropout, BatchNormalization, GlobalAveragePooling2D
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.applications import MobileNet
from tensorflow.keras.applications.densenet import DenseNet121
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, ReduceLROnPlateau

# Mengabaikan peringatan
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

# Mencetak versi TensorFlow yang sedang digunakan
print(tf.__version__)

!pip freeze > requirements.txt

"""## Data Preparation

### Data Loading
"""

# Import module yang disediakan google colab untuk kebutuhan upload file
from google.colab import files
files.upload()

# Download kaggle dataset and unzip the file
# !cp kaggle.json ~/.kaggle/
# !chmod 600 ~/.kaggle/kaggle.json
!kaggle datasets download -d alxmamaev/flowers-recognition
!mkdir -p /content/flowers-recognition
!unzip flowers-recognition.zip -d /content/flowers-recognition

from google.colab import drive
drive.mount('/content/drive')

"""### Data Preprocessing"""

# Path dataset
dataset_dir = "/content/flowers-recognition/flowers"
classes = ['daisy', 'dandelion', 'rose', 'sunflower', 'tulip']

# Analisis distribusi data
data_info = []
for flower in classes:
    folder_path = os.path.join(dataset_dir, flower)
    file_list = os.listdir(folder_path)
    data_info.extend([(flower, file) for file in file_list])

# Konversi data menjadi DataFrame
df = pd.DataFrame(data_info, columns=['class', 'filename'])

# Tampilkan beberapa baris awal
print("Preview dataset:")
print(df.head())

# Informasi dataset
print("\nInformasi dataset:")
print(df.info())

# Deskripsi statistik dataset
print("\nDeskripsi statistik dataset:")
print(df.describe())

# Cek missing values
missing_values = df.isnull().sum()
print("\nNilai hilang di setiap kolom:")
print(missing_values)

# Cek distribusi dataset
print("Distribusi dataset per kelas:")
print(df['class'].value_counts())

# Visualisasi distribusi label
data_count = df['class'].value_counts()
plt.figure(figsize=(8, 5))
sns.barplot(x=data_count.index, y=data_count.values, palette='Set3')
plt.title('Distribusi Data pada Setiap Kelas')
plt.xlabel('Kelas')
plt.ylabel('Jumlah Gambar')
plt.xticks(rotation=30)
plt.show()

"""### Visualisasi Gambar dari Setiap Kelas"""

# Visualisasi contoh gambar dari setiap kelas
plt.figure(figsize=(15, 10))
for i, flower in enumerate(classes):
    folder_path = os.path.join(dataset_dir, flower)
    sample_image = os.path.join(folder_path, os.listdir(folder_path)[0])
    image = cv2.imread(sample_image)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

    plt.subplot(2, 3, i + 1)
    plt.imshow(image)
    plt.title(flower)
    plt.axis('off')
plt.tight_layout()
plt.show()

output_dir = "/content/flower_dataset_split"

# Buat folder untuk train dan test
train_dir = os.path.join(output_dir, "train")
test_dir = os.path.join(output_dir, "test")

os.makedirs(train_dir, exist_ok=True)
os.makedirs(test_dir, exist_ok=True)

classes = ['daisy', 'dandelion', 'rose', 'sunflower', 'tulip']

# Buat subfolder untuk setiap kelas di train dan test
for class_name in classes:
    os.makedirs(os.path.join(train_dir, class_name), exist_ok=True)
    os.makedirs(os.path.join(test_dir, class_name), exist_ok=True)

# Proporsi data untuk train dan test
train_ratio = 0.8


# Bagi dataset
for class_name in classes:
    class_folder = os.path.join(dataset_dir, class_name)
    images = os.listdir(class_folder)
    random.shuffle(images)

    train_count = int(len(images) * train_ratio)

    train_images = images[:train_count]
    test_images = images[train_count:]

    # Pindahkan gambar ke folder train
    for img in train_images:
        src_path = os.path.join(class_folder, img)
        dst_path = os.path.join(train_dir, class_name, img)
        shutil.copy(src_path, dst_path)

    # Pindahkan gambar ke folder test
    for img in test_images:
        src_path = os.path.join(class_folder, img)
        dst_path = os.path.join(test_dir, class_name, img)
        shutil.copy(src_path, dst_path)

print("Dataset telah dibagi menjadi train dan test!")

train_datagen = ImageDataGenerator(
    rescale=1./255,       # Normalisasi pixel (0-1)
    rotation_range=30,    # Rotasi acak
    width_shift_range=0.2,# Pergeseran horizontal
    height_shift_range=0.2,# Pergeseran vertikal
    shear_range=0.2,      # Distorsi geser
    zoom_range=0.2,       # Zoom acak
    horizontal_flip=True, # Flip horizontal
    fill_mode='nearest'   # Isi piksel kosong
)

test_datagen = ImageDataGenerator(rescale=1./255)

# Load Dataset
train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(150, 150),
    batch_size=32,
    class_mode='categorical'
)

validation_generator = test_datagen.flow_from_directory(
    test_dir,
    target_size=(150, 150),
    batch_size=32,
    class_mode='categorical'
)

"""## Modelling"""

# Load pre-trained VGG16 model without the fully connected layers
def create_model():
    base_model = VGG16(weights='imagenet', include_top=False, input_shape=(150, 150, 3))

    # Fine-tune the last 4 layers
    for layer in base_model.layers[:-4]:
        layer.trainable = False

    # Build the model
    model = Sequential([
        base_model,
        Conv2D(64, (3, 3), activation='relu'),
        BatchNormalization(),
        MaxPooling2D(pool_size=(2, 2)),
        GlobalAveragePooling2D(),
        Dense(256, activation='relu'),
        Dropout(0.5),
        Dense(train_generator.num_classes, activation='softmax')
    ])

    # Compile the model
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )

    return model

# Create the model
model = create_model()
model.summary()

# Callbacks
early_stopping = EarlyStopping(
    monitor='val_accuracy',
    patience=5,
    restore_best_weights=True
)

reduce_lr = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.2,
    patience=3,
    min_lr=1e-5
)

# Train the model
history = model.fit(
    train_generator,
    validation_data=validation_generator,
    epochs=25,
    callbacks=[early_stopping, reduce_lr]
)

"""## Evaluasi dan Visualisasi"""

# Plot Accuracy and Loss
plt.figure(figsize=(12, 5))

# Accuracy Plot
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Loss Plot
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

"""## Konversi Model"""

# SavedModel
save_path = 'model'
tf.saved_model.save(model, save_path)

# TF-Lite
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()

tflite_model_dir = 'tf_lite_model'
if not os.path.exists(tflite_model_dir):
    os.makedirs(tflite_model_dir)

tflite_model_path = os.path.join(tflite_model_dir, 'model.tflite')
with open(tflite_model_path, 'wb') as f:
    f.write(tflite_model)

model.save('model_name.h5')

# TFJS
!pip install tensorflowjs

# Convert model.h5 to model
!tensorflowjs_converter --input_format=keras model_name.h5 tfjs_model

"""## Inference (Optional)"""

